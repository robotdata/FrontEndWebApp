<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <!-- StyleSheets -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="static/css/style.css">
    <!-- <link rel="stylesheet" href="/static/css/products.css"> -->
    <!-- Font Awesome -->
    <link href="static/css/all.css" rel="stylesheet">


    <title>3D Overlay on 2D Image</title>
  </head>
  <body>
   <main>
       
<section id = "title">
    <div id="mySidenav" class="sidenav">
        <a href="javascript:void(0  )" class="closebtn" onclick="closeNav()">&times;</a>
        <a href="index.html">Home</a>
        <a href="#Docker">Why Docker</a>
        <a href="#Dope">Dope</a>
        <a href="#AAE">AAE</a>
        <a href="#Graspnet">Graspnet</a>
        <a href ="#SSD">SSD</a>
        <a href="#Detectron2">Detectron2</a>
        <a href="contactus.html">Contact Us</a>
      </div>
</section>
<section id = "">
    <section id = "main">                            
    <section id = "Docker">
        <h2>Why Docker?</h2>
        <p>Docker facilitates the application of algorithms and methods by packaging the code, framework and other necessary dependencies in the container. We provide the docker images for the machine learning algorithms so that they can be tested or used by the users w in a more convenient way.</p>
        <p>To understand more : <a href = "https://www.docker.com/resources/what-container">Click here</a></p>
        <!-- <ul>
            <li>Install docker and nvidia docker on the host computer</li>
            <li>Run xhost +local:root on the host computer to enable using GUI with docker.
                <a href = "http://wiki.ros.org/docker/Tutorials/GUI">Tutorial</a></li>
            <li>docker pull registry.gitlab.com/haiandaidi/docker_graspnet:2020_07_03_ros</li>
            <li>git pull https://gitlab.com/haiandaidi/docker_graspnet.git</li>
            <li>docker run --gpus all -it --network=host --env="DISPLAY" -v /tmp/.X11-unix:/tmp/.X11-unix:rw -v $PWD/graspnet_ws:/graspnet_ws:rw -w /graspnet_ws registry.gitlab.com/haiandaidi/docker_graspnet:2020_07_03_ro</li>
            <li>To run the demo:
                cd /graspnet_ws/src/graspnet/pytorch_6dof-graspnet
                python3 -m demo.main</li>
            <li>Per default, the demo script runs the GAN sampler with sampling based refinement. To use the VAE sampler and/or gradient refinement run:
                python3 -m demo.main --grasp_sampler_folder checkpoints/vae_pretrained/ --refinement_method gradient</li>
            <li>The input data is in the folder of demo/data/
                Closing the opened window will let the algorithm go to process next input.
                To check the generated grasp information, read demo/main.py</li>
        </ul>
        <h4>ROS Wrapper</h2>
        <ul>
            <li>cd /graspnet_ws</li>
            <li>catkin_make</li>
            <li>source devel/setup.bash</li>
            <li>rosrun graspnet generate_grasp_poses.py</li>
            <li>input topic: 'object_pc', PointCloud2 
                This is pointcloud of the object to be grasped.</li>
            <li>output topic: 'generated_grasp_posearray', PoseArray 
                This include all the generated grasp poses.</li>
        </ul>
        <p>These parameters can be changed in script_dev/generate_grasp_poses.py</p>
    <section id = "AAE">
        <hr>
        <h2>Docker AAE</h2>
        <hr> -->
    </section>
    <section id = "Dope">
        <hr>
        <h2>Docker Dope</h2>
        <hr>
        <p>6D Object pose estimation is the task of detecting the 6D pose of an object, which includes its location and orientation.
        </p>
        <p>This is essential for many robotic applications, such as grasping, movement prediction, and status estimation.</p>
        <p>DOPE method is one of such implementations and detects the object 6D pose from one RGB image. Synthetic data is used in the implementation, thus saves the effort for annotation of real data. With the training of synthetic data, the method is capable of detecting from real images precisely.</p>
        <p>The implementation  is based on <a href = "https://github.com/NVlabs/Deep_Object_Pose">https://github.com/NVlabs/Deep_Object_Pose</a>
            Copyright (C) 2018 NVIDIA Corporation. All rights reserved. Licensed under the CC BY-NC-SA 4.0 license.</p>
		<div class = "row">
			<div class = "col-md-6">
				<img class = "custom_image" src = "static/Images/Dope.png">
			</div>
			<div class = "col-md-6">
			<div class = "video_parent">
        <div class = "video_child">
            <video width = "100%" height = "30%"
            src = "static/Images/DOPE_demo.mp4" controls>    
        </div>
    </div>
    </section>
    <section id = "AAE">
        <hr>
        <h2>AAE</h2>
        <hr>
        <p>AAE method can also detect the object 6D pose from one RGB image. The accuracy can be improved by using additional depth information. Similar to the DOPE method, the AAE implementation also uses synthetic data for training. An object detection algorithm is needed as part of the whole method. In our example implementation, the SSD method is used which can also be found in another docker image.</p>
        <p>The implementation is based on <a href = "https://github.com/DLR-RM/AugmentedAutoencoder">https://github.com/DLR-RM/AugmentedAutoencoder</a></p>
        <h5>Isaac SDK Vs AAE</h5>
        <p>In ISAAC SDK, there is also an implementation of AAE method. But till the version of 2019.3, the generated synthetic data is totally based on the virtual simulation environment. After testing, we found the resulting neural network does not generate well in real environments, for example a room with white walls and doors as background. The chosen implementation trains the augmented autoencoder with lots of synthetic data taking real images as background. The resulting neural network performs properly in real environments.</p>
        <h4>AAE Implementation in real environments</h4>
        <img class="custom_image" src = "static/Images/AAEreal.png">
        <h4>AAE of ISAAC SDK. Performing in the simulation environment</h4>
        <img class = "custom_image" src = "static/Images/tello.JPG"> 
    </section>
    <section id= "Graspnet">
        <hr>
        <h2>6-DoF Graspnet</h2>
        <hr>
        <p>The grasping ability enables robots to interact with the world objects directly. Traditional robotic grasping frameworks first estimate the pose of the object to be grasped and then calculate the potential grasping poses using related grasping generation algorithms. </p>
        <p>This machine learning based implementation generates the grasping poses directly from the point cloud input. The grasping scores for all the potential grasps are also generated for further processing.</p>
        <p>The implementation is based on <a href = "https://github.com/jsll/pytorch_6dof-graspnet">https://github.com/jsll/pytorch_6dof-graspnet</a></p>
        <p>The source code is released under MIT License and the trained weights are released under <a href = "https://github.com/NVlabs/6dof-graspnet/blob/master/TRAINED_MODEL_LICENSE">CC-BY-NC-SA 2.0</a></p>
        <img class = "custom_image" src = "static/Images/Graspnet.png">
    </section>
    <section id = "SSD">
        <hr>
        <h3>SSD</h3>
        <hr>
        <p>Object detection is the task to detect instances of semantic objects of a certain class (such as humans, flowers, or cars) in images. SSD object detection is designed for real-time applications with a high processing frequency. For the AAE 6D pose estimation, an object detection algorithm is needed as part of the whole method. In our example implementation, this SSD method is used.</p>
        <p>The implementation is based on <a href = "https://github.com/naisy/train_ssd_mobilenet">https://github.com/naisy/train_ssd_mobilenet</a> based on MIT License</p>
        <img class = "custom_image" src = "static/Images/ssd.png">
    </section>
    <section id = "Detectron2">
        <hr>
        <h3>Detectron2</h3>
        <hr>
        <p>The implementation is based on <a href = "https://github.com/facebookresearch/detectron2">https://github.com/facebookresearch/detectron2</a></p>
        <p>Detectron2 is released under the Apache 2.0 license<a href = "https://github.com/facebookresearch/detectron2/blob/master/LICENSE">Apache 2.0 license</a></p>
        <p>It is an implementation of state-of-the-art object detection algorithms. It supports features like object detection, instance segmentation, panoptic segmentation, densepose, etc.</p>
        <p>The ROS implementation is based on <a href = "https://github.com/DavidFernandezChaves/Detectron2_ros">https://github.com/DavidFernandezChaves/Detectron2_ros</a></p>
        <img class = "custom_image" src = "static/Images/detectron2final.png">
    </section>
</section>
</section>

   </main>
    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.6/umd/popper.min.js" integrity="sha384-wHAiFfRlMFy6i5SRaxvfOCifBUQy1xHdJ/yoi7FRNXMRBu5WHdZYu1hA6ZOblgut" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.2.1/js/bootstrap.min.js" integrity="sha384-B0UglyR+jN6CkvvICOB2joaf5I4l3gm9GU6Hc1og6Ls7i6U/mkkaduKaBhlAXv9k" crossorigin="anonymous"></script>
      <!-- Import our custom JavaScript -->
  <script src="static/js/app.js"></script>
  <script defer src="static/js/all.js"></script>
    
  </body>
</html>